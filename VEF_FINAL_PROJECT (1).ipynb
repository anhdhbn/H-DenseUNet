{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VEF FINAL PROJECT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJLsR_ynvv1K",
        "colab_type": "code",
        "outputId": "51b7ad7c-b185-42bd-b25a-50dff8fd4650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd H-DenseUNet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'H-DenseUNet'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gxectxX2KsM",
        "colab_type": "code",
        "outputId": "f1443a02-e4e9-449f-8a1a-446ce0792efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!git clone https://github.com/anhdhbn/H-DenseUNet.git\n",
        "%cd H-DenseUNet\n",
        "%mkdir /content/H-DenseUNet/data\n",
        "%mkdir /content/H-DenseUNet/data/TrainingData\n",
        "%mkdir /content/H-DenseUNet/data/TestData\n",
        "!mkdir model\n",
        "!pip install medpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'H-DenseUNet'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 888 (delta 51), reused 42 (delta 20), pack-reused 807\u001b[K\n",
            "Receiving objects: 100% (888/888), 13.79 MiB | 13.33 MiB/s, done.\n",
            "Resolving deltas: 100% (361/361), done.\n",
            "/content/H-DenseUNet\n",
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from medpy) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python2.7/dist-packages (from medpy) (1.16.4)\n",
            "Collecting SimpleITK>=1.1.0 (from medpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/de/db2364176b9066bee07b1e8a5ff084b5b46389de881b1674cc20718f6ece/SimpleITK-1.2.0-cp27-cp27mu-manylinux1_x86_64.whl (42.5MB)\n",
            "\u001b[K     |████████████████████████████████| 42.5MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: medpy\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "Successfully built medpy\n",
            "Installing collected packages: SimpleITK, medpy\n",
            "Successfully installed SimpleITK-1.2.0 medpy-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pZSpQjttCSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cp -r \"/content/drive/My Drive/Vef project/DenseNet161 Weights/densenet161_weights_tf.h5\" \"/content/H-DenseUNet/model/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcvFLyIa2m7X",
        "colab_type": "code",
        "outputId": "52de89b9-dd0d-4dae-c5fe-91edf1737b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN5sEpi_kRUH",
        "colab_type": "code",
        "outputId": "5fff784d-70fc-4388-b846-36fe945f9b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "# sys.path.insert(0,'Keras-2.0.8')\n",
        "from multiprocessing.dummy import Pool as ThreadPool\n",
        "import random\n",
        "from medpy.io import load\n",
        "import numpy as np\n",
        "import argparse\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras.backend as K\n",
        "from loss import weighted_crossentropy_2ddense\n",
        "import os\n",
        "import math\n",
        "\n",
        "from keras.utils import multi_gpu_model\n",
        "\n",
        "from denseunet import DenseUNet\n",
        "from skimage.transform import resize\n",
        "K.set_image_dim_ordering('tf')\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNqhT8BBkX1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A(object):\n",
        "    pass\n",
        "  \n",
        "\n",
        "args = A()\n",
        "args.data = \"/content/drive/My Drive/Vef project/data\"\n",
        "args.save_path = \"/content/H-DenseUNet/Experiments\"\n",
        "args.b = 4\n",
        "args.input_size = 224\n",
        "args.model_weight = './model/densenet161_weights_tf.h5'\n",
        "args.input_cols = 3\n",
        "args.mean = 48\n",
        "args.thread_num = 14\n",
        "\n",
        "\n",
        "MEAN = args.mean\n",
        "thread_num = args.thread_num\n",
        "\n",
        "liverlist = [32,34,38,41,47,87,89,91,105,106,114,115,119]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U98y_fWAm8LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_seq_crop_data_masktumor_try(Parameter_List):\n",
        "    img = Parameter_List[0]\n",
        "    tumor = Parameter_List[1]\n",
        "    lines = Parameter_List[2]\n",
        "    numid = Parameter_List[3]\n",
        "    minindex = Parameter_List[4]\n",
        "    maxindex = Parameter_List[5]\n",
        "    #  randomly scale\n",
        "    scale = np.random.uniform(0.8,1.2)\n",
        "    deps = int(args.input_size * scale)\n",
        "    rows = int(args.input_size * scale)\n",
        "    cols = 3\n",
        "\n",
        "    sed = np.random.randint(1,numid)\n",
        "    cen = lines[sed-1]\n",
        "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
        "\n",
        "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
        "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
        "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
        "    cropp_img = img[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                c - cols / 2: c + cols / 2 + 1].copy()\n",
        "    cropp_tumor = tumor[a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
        "                  c - cols / 2:c + cols / 2 + 1].copy()\n",
        "\n",
        "    cropp_img -= MEAN\n",
        "     # randomly flipping\n",
        "    flip_num = np.random.randint(0, 8)\n",
        "    if flip_num == 1:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "    elif flip_num == 2:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "    elif flip_num == 3:\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 4:\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 5:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
        "    elif flip_num == 6:\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "        cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
        "        cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
        "    elif flip_num == 7:\n",
        "        cropp_img = np.flipud(cropp_img)\n",
        "        cropp_tumor = np.flipud(cropp_tumor)\n",
        "        cropp_img = np.fliplr(cropp_img)\n",
        "        cropp_tumor = np.fliplr(cropp_tumor)\n",
        "\n",
        "    cropp_tumor = resize(cropp_tumor, (args.input_size,args.input_size,args.input_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
        "    cropp_img   = resize(cropp_img, (args.input_size,args.input_size,args.input_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
        "    return cropp_img, cropp_tumor[:,:,1]\n",
        "\n",
        "def generate_arrays_from_file(batch_size,trainidx):\n",
        "    while 1:\n",
        "        X = np.zeros((batch_size, args.input_size, args.input_size, args.input_cols), dtype='float32')\n",
        "        Y = np.zeros((batch_size, args.input_size, args.input_size, 1), dtype='int16')\n",
        "        Parameter_List = []\n",
        "        for idx in xrange(batch_size):\n",
        "            count = random.choice(trainidx)\n",
        "\n",
        "            img, img_header = load(args.data+ '/myTrainingData/volume-' + str(count) + '.nii')\n",
        "            tumor, tumor_header = load(args.data + '/myTrainingData/segmentation-' + str(count) + '.nii')\n",
        "            maxmin = np.loadtxt(args.data + '/myTrainingDataTxt/LiverBox/box_' + str(count) + '.txt', delimiter=' ')\n",
        "\n",
        "            minindex = maxmin[0:3]\n",
        "            maxindex = maxmin[3:6]\n",
        "            minindex = np.array(minindex, dtype='int')\n",
        "            maxindex = np.array(maxindex, dtype='int')\n",
        "            minindex[0] = max(minindex[0] - 3, 0)\n",
        "            minindex[1] = max(minindex[1] - 3, 0)\n",
        "            minindex[2] = max(minindex[2] - 3, 0)\n",
        "            maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
        "            maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
        "            maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
        "            \n",
        "            f1 = open(args.data + '/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt', 'r')\n",
        "            tumorline = f1.readlines()\n",
        "            f1.close()\n",
        "            \n",
        "            f2 = open(args.data + '/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt', 'r')\n",
        "            liverline = f2.readlines()\n",
        "            f2.close()\n",
        "            \n",
        "            num = np.random.randint(0,6)\n",
        "            if num < 3 or (count in liverlist):\n",
        "                lines = liverline\n",
        "                numid = len(liverline)\n",
        "            else:\n",
        "                lines = tumorline\n",
        "                numid = len(tumorline)\n",
        "            Parameter_List.append([img, tumor, lines, numid, minindex, maxindex])\n",
        "        pool = ThreadPool(thread_num)\n",
        "        result_list = pool.map(load_seq_crop_data_masktumor_try, Parameter_List)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        for idx in xrange(len(result_list)):\n",
        "            X[idx, :, :, :] = result_list[idx][0]\n",
        "            Y[idx, :, :, 0] = result_list[idx][1]\n",
        "        # print(\"getsizeof X, y\", sys.getsizeof(X)*1.0/1024/1024, sys.getsizeof(Y)*1.0/1024/1024)\n",
        "        yield (X,Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlrW3t3kIVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_fast_files(args, trainidx):  \n",
        "    img_list = []\n",
        "    tumor_list = []\n",
        "    minindex_list = []\n",
        "    maxindex_list = []\n",
        "    tumorlines = []\n",
        "    tumoridx = []\n",
        "    liveridx = []\n",
        "    liverlines = []\n",
        "    number_sample = 0\n",
        "    for idx in trainidx:\n",
        "        print(idx)\n",
        "        img, img_header = load(args.data+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
        "        (x, y, z) = img.shape\n",
        "        number_sample = int(z) + int(number_sample)\n",
        "    return number_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyBNRY55nIWn",
        "colab_type": "code",
        "outputId": "2671c7bb-3085-4384-b8b3-2059e3000e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "    args.b = 3\n",
        "    number_train = 11\n",
        "    trainidx = list(range(number_train))\n",
        "    validx = list(range(31,  33))\n",
        "    number_samples = load_fast_files(args, trainidx)\n",
        "    number_samples_val = load_fast_files(args, validx)\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "\n",
        "    \n",
        "\n",
        "    model = DenseUNet(reduction=0.5, args=args,nb_dense_block=4, growth_rate=12,nb_filter=24)\n",
        "    # model.load_weights(args.model_weight, by_name=True)\n",
        "    # model = make_parallel(model, args.b / 10, mini_batch=10)\n",
        "    # model = multi_gpu_model(model, gpus=None)\n",
        "    sgd = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss=[weighted_crossentropy_2ddense], metrics=['accuracy'])\n",
        "#     model.summary()\n",
        "    \n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model......')\n",
        "    print('-'*30)\n",
        "\n",
        "    if not os.path.exists(args.save_path):\n",
        "        os.mkdir(args.save_path)\n",
        "\n",
        "    if not os.path.exists(args.save_path + \"/model\"):\n",
        "        os.mkdir(args.save_path + '/model')\n",
        "        os.mkdir(args.save_path + '/history')\n",
        "    else:\n",
        "        if os.path.exists(args.save_path+ \"/history/lossbatch.txt\"):\n",
        "            os.remove(args.save_path + '/history/lossbatch.txt')\n",
        "        if os.path.exists(args.save_path + \"/history/lossepoch.txt\"):\n",
        "            os.remove(args.save_path + '/history/lossepoch.txt')\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(args.save_path + '/model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
        "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 1)\n",
        "\n",
        "\n",
        "#     steps = math.ceil(number_samples / args.b)\n",
        "    steps = 150\n",
        "    steps_val = math.ceil(number_samples_val / args.b)\n",
        "    print(\"steps\", int(steps))\n",
        "    print(\"steps_val\", int(steps_val))\n",
        "    # model.fit_generator(generate_arrays_from_file(args.b, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx,\n",
        "    #                                               liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
        "    #                                                 epochs= 6000, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
        "    #                                                 workers=3, use_multiprocessing=True)\n",
        "    history = model.fit_generator(generate_arrays_from_file(args.b, trainidx),steps_per_epoch=int(steps),validation_data=generate_arrays_from_file(args.b, validx),\n",
        "                                                    validation_steps=steps_val,\n",
        "                                                    epochs= 3, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
        "                                                    workers=1, use_multiprocessing=False)\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    print ('Finised Training .......')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "31\n",
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0711 07:20:32.778115 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0711 07:20:32.865375 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0711 07:20:32.917412 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0711 07:20:32.918646 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0711 07:20:32.924813 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Creating and compiling model...\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0711 07:20:36.397550 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0711 07:20:36.511140 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0711 07:20:37.981086 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0711 07:20:55.549961 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0711 07:20:56.031157 140596558296960 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0711 07:20:56.263196 140596558296960 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0711 07:20:56.279864 140596558296960 deprecation.py:323] From loss.py:36: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Fitting model......\n",
            "------------------------------\n",
            "steps 150\n",
            "steps_val 76\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "150/150 [==============================] - 1951s 13s/step - loss: 0.4678 - acc: 0.9715 - val_loss: 0.3638 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00001: saving model to /content/H-DenseUNet/Experiments/model/weights.01-0.47.hdf5\n",
            "Epoch 2/3\n",
            " 46/150 [========>.....................] - ETA: 17:13 - loss: 0.3588 - acc: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCAVLULOu_Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history.history['acc']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}